{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11주차 과제 .ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "• 자신이 원하는 영화의 리뷰데이터를 수집하여 Word2Vec 모델 을 만들고 특정 단어를 입력하여 유사 단어를 나열해보시오."
      ],
      "metadata": {
        "id": "pinfPCqFIm2g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJQZoPNv0bYh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd \n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#영화 기생충 리뷰데이터\n",
        "\n",
        "pres=['https://movie.naver.com/movie/bi/mi/pointWriteFormList.naver?code=161967&type=after&onlyActualPointYn=N&onlySpoilerPointYn=N&order=highest&page=1',\n",
        "'https://movie.naver.com/movie/bi/mi/pointWriteFormList.naver?code=161967&type=after&onlyActualPointYn=N&onlySpoilerPointYn=N&order=lowest&page=1']\n",
        "\n",
        "review = []\n",
        "rate =[]\n",
        "target=[]\n",
        "\n",
        "for pre in pres:\n",
        "    for i in range(1,400):\n",
        "        url=pre+str(i)\n",
        "        res=requests.get(url)\n",
        "        soup=BeautifulSoup(res.content,'html.parser')\n",
        "        \n",
        "        id_list=[]\n",
        "        id_pre='_filtered_ment_'\n",
        "        \n",
        "        for i in range(10):\n",
        "            id_list.append(id_pre+str(i))\n",
        "        \n",
        "        for id in id_list:\n",
        "            review.append(soup.find('span',{'id':id}).get_text().strip())\n",
        "            \n",
        "        rate_list =[]\n",
        "        rate_list =(soup.select('div.star_score > em'))\n",
        "        \n",
        "        for i in range(10):\n",
        "            r = int(re.sub('<.+?>','',str(rate_list[i])))\n",
        "            rate.append(r)\n",
        "            if r>=8:\n",
        "                target.append(1)\n",
        "            elif r<=4:\n",
        "                target.append(0)\n",
        "            else:\n",
        "                target.append(-1)\n",
        "        \n",
        "\n",
        "df=pd.DataFrame({'review':review,'rate':rate,'target':target})\n",
        "df"
      ],
      "metadata": {
        "id": "dZD8fFAR0p9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('review_data.csv')"
      ],
      "metadata": {
        "id": "9eXMSVaoAeBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Null값이 존재하는 행 제거 & 존재하는지 확인 \n",
        "train_data = pd.read_csv('review_data.csv')\n",
        "train_data = train_data.dropna(how = 'any')\n",
        "print(train_data.isnull().values.any())\n"
      ],
      "metadata": {
        "id": "odI9H6sfAjrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#한글이 아닌 경우 제외\n",
        "train_data['review'] = train_data['review'].str.replace(\"[^ㄱ-ㅎ ㅏ-ㅣ 가-힣]\",\"\")\n",
        "train_data"
      ],
      "metadata": {
        "id": "ZJdoLNYvA3Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords= ['으로','이다','ㅋㅋ','그래서','ㅎㅎ','ㅠㅠ','습니다','있다','했는지','있게','ㅠ','~~','^^','이라는걸','이해','있었다','한다','인다','않다','그리고',\n",
        "            '의','가','로','에서','진짜','이고','없다','으로','정말','영화','하다','입니다','합니다','씩','때','에','차라리','오다','왔다','같다','에요','니다',\n",
        "            '했다','그냥','가장','에게','까지','진짜','싶다','보다','했습니다', '된다','봤는데','본다', '했는데','아쉽다','역시','ㄹㅇ','ㅜ','처음','ㄷㄷ',';;;',\n",
        "            '그리고','너무','느끼다','부터','생각','!!!','~~~','근데','팝콘','정도','하고','한테','아니다','되어다','안된다','인데','어떻다','인가','라고','처럼',\n",
        "            '되다','깊다','남다','받다','대한','보이다','남다','나다','들다','이렇다','아주','때문','맞다','가다','밖에','나오다','하나','보지','라는','지다','주다',\n",
        "            '내다','라는','알다','에는','이라','하지만','말다','맞다','넘다','라는','자다','지만','이렇다','이나']"
      ],
      "metadata": {
        "id": "5iXD5BTdFOmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#토큰화 작업 \n",
        "okt = Okt()\n",
        "tokenized_data = []\n",
        "\n",
        "for sentence in train_data ['review']:\n",
        "    temp_X = okt.morphs(sentence,stem = True)\n",
        "    #불용어제거, 2글자 이상의 단어만 추출\n",
        "    temp_X = [word for word in temp_X if not word in stopwords]\n",
        "    temp_X = [word for word in temp_X if len(word)>1]\n",
        "    tokenized_data.append(temp_X)"
      ],
      "metadata": {
        "id": "aWgvMQ0qBvIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"리뷰의 최대 길이:\", max(len(l) for l in tokenized_data))\n",
        "print(\"리뷰의 평균 길이:\", sum(map(len, tokenized_data))/ len(tokenized_data))\n",
        "\n",
        "#histogram graph\n",
        "plt.hist([len(s) for s in tokenized_data], bins = 50)\n",
        "plt.xlabel('lenght of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C7O6Xn5YCKRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences = tokenized_data, size = 100, window = 5, min_count = 5, workers = 4, sg=0)\n",
        "#완성된 메트릭스 크기 확인 \n",
        "model.wv.vectors.shape"
      ],
      "metadata": {
        "id": "mM_fe7Y-CWnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10개의 단어 유사도 확인\n",
        "for i in range(10):\n",
        "    result = model.wv.most_similar([input()])\n",
        "    print(result)\n",
        "    print(\"-\"*120)"
      ],
      "metadata": {
        "id": "txbdce4_LAPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#전처리된 단어들 그래프로 표현하기\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import font_manager, rc"
      ],
      "metadata": {
        "id": "tEuAJ7vHODA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#문장을 이용하여 단어와 백터를 생성, 단어백터를 구함\n",
        "sentences= []\n",
        "\n",
        "for i in range(1,2):\n",
        "    sentences.append(tokenized_data[i])\n",
        "\n",
        "model = Word2Vec(sentences = tokenized_data, size = 100, min_count = 70, workers = 4, sg=0)\n",
        "word_vectors = model.wv\n",
        "\n",
        "vocabs = word_vectors.vocab.keys()\n",
        "word_vectors_list = [word_vectors[v] for v in vocabs]"
      ],
      "metadata": {
        "id": "LcQ8iAFmOVeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocabs)"
      ],
      "metadata": {
        "id": "m37PSjwSSMzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 2)\n",
        "xys = pca.fit_transform(word_vectors_list)\n",
        "xs = xys[:,0]\n",
        "ys = xys[:,1]"
      ],
      "metadata": {
        "id": "5eYoQmXBSezG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "print ('캐시 위치: ', mpl.get_cachedir())\n",
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "metadata": {
        "id": "4AvFT9NTTr-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', family='NanumBarunGothic') "
      ],
      "metadata": {
        "id": "QAiTQvNHW1pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "font = font_manager.FontProperties(fname=font_path).get_name()\n",
        "rc('font',family=font)\n",
        "\n",
        "plt.figure(figsize = (20,13))\n",
        "plt.scatter(xs,ys,marker = 'o')\n",
        "for i, v in enumerate(vocabs):\n",
        "    plt.annotate(v,xy=(xs[i],ys[i]))"
      ],
      "metadata": {
        "id": "AZxyVB1tSugN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EyUbOIJCqBSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot \n",
        "from numpy.linalg import norm \n",
        "import numpy as np \n",
        "def cos_sim(A,B):\n",
        "    return dot (A,B)/(norm(A)*norm(B))\n",
        "\n",
        "doc1=np.array([3,4])\n",
        "doc2=np.array([-1,2])\n",
        "\n",
        "print(cos_sim(doc1, doc2))"
      ],
      "metadata": {
        "id": "r4_aPBukW0Og"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}